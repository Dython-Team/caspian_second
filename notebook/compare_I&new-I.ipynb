{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import statsmodels.api as sm\n",
    "from scipy.interpolate import interp1d\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_i_antro= pd.read_spss('data/Anthropometry-Caspian.sav',convert_categoricals=True)\n",
    "df_i_next = pd.read_spss('data/caspian1 data.sav',convert_categoricals=True)\n",
    "df_i_prev = pd.read_spss('data/CASPIANI.sav',convert_categoricals=True) \n",
    "\n",
    "def rename_features(df1,is_new=False):\n",
    "    df2 = df1.copy()\n",
    "    # Rename the features in the dataframe\n",
    "    if is_new:\n",
    "        df2.columns = df2.columns.str.lower()\n",
    "        df2.rename(columns={'univer': 'university','district':'region', 'schoolty':'schoolType', 'wc':'waist'}, inplace=True)      \n",
    "    else:\n",
    "        df2.columns = df2.columns.str.lower()\n",
    "        df2.rename(columns={'univer': 'university','district':'region', 'schoolty':'schoolType'}, inplace=True)      \n",
    "    return df2\n",
    "\n",
    "df_i_prev=rename_features(df_i_prev)\n",
    "df_i_antro=rename_features(df_i_antro)\n",
    "df_i_next=rename_features(df_i_next)\n",
    "\n",
    "# Now define df_dict with the modified dataframes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'code', 'sex', 'university', 'state', 'province', 'region',\n",
       "       'schoolType', 'grade', 'birthdat',\n",
       "       ...\n",
       "       'tg98', 'hdl50', 'fbs100', 'wc75', 'dbp_sbp2', 'tg150', 'hdl35', 'wc2',\n",
       "       'ms_a', 'ms_b'],\n",
       "      dtype='object', length=318)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_i_prev.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'code', 'sex', 'university', 'state', 'province', 'region',\n",
       "       'schoolType', 'grade', 'birthdat', 'age', 'fjob', 'feducati', 'mjob',\n",
       "       'meducati', 'househol', 'housing', 'weight', 'height', 'waist', 'hip',\n",
       "       'bmi', 'fbs_rec', 'tc_r', 'tg_rec', 'tc_rec', 'ldl_re', 'hdl_rec',\n",
       "       'bmi_rec'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_i_antro.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_i_antro' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m a \u001b[38;5;241m=\u001b[39m \u001b[43mdf_i_antro\u001b[49m\u001b[38;5;241m.\u001b[39muniversity\u001b[38;5;241m.\u001b[39munique()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df_i_antro' is not defined"
     ]
    }
   ],
   "source": [
    "a = df_i_antro.university.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('university_unique2.txt', 'w') as f:\n",
    "    for item in a:\n",
    "        f.write(f\"{item}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of records with NaN value in weight or height in caspian_I_prev: 11\n",
      "Number of records with NaN value in weight or height in caspian_I_antro: 132\n",
      "Number of records with NaN value in weight or height in caspian_I_next: 132\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_137271/3365927308.py:67: FutureWarning: The behavior of Series.replace (and DataFrame.replace) with CategoricalDtype is deprecated. In a future version, replace will only be used for cases that preserve the categories. To change the categories, use ser.cat.rename_categories instead.\n",
      "  processed_dfs['caspian_I_prev']['university'] = processed_dfs['caspian_I_prev']['university'].replace(university_to_province)\n",
      "/tmp/ipykernel_137271/3365927308.py:74: FutureWarning: The behavior of Series.replace (and DataFrame.replace) with CategoricalDtype is deprecated. In a future version, replace will only be used for cases that preserve the categories. To change the categories, use ser.cat.rename_categories instead.\n",
      "  processed_dfs['caspian_I_antro']['university'] = processed_dfs['caspian_I_antro']['university'].replace(university_to_province)\n"
     ]
    }
   ],
   "source": [
    "df_dict = {'caspian_I_prev': df_i_prev,'caspian_I_antro': df_i_antro, 'caspian_I_next': df_i_next}\n",
    "\n",
    "def preprocess(dataframes_dict):\n",
    "    processed_dfs = {}  # Dictionary to store processed DataFrames\n",
    "    for name, df_org in dataframes_dict.items():\n",
    "        df = df_org.copy()\n",
    "\n",
    "        # Filter age\n",
    "        df = df[(df[\"age\"] >= 7) & (df[\"age\"] <= 18)]\n",
    "        # please change the type of heught_1 and weight_1 in caspian4 to numeric if you can't do it directly uncomment two line below\n",
    "        df['height'] = pd.to_numeric(df['height'], errors='coerce')\n",
    "        df['weight'] = pd.to_numeric(df['weight'], errors='coerce')\n",
    "        df[\"bmi1\"] = df[\"weight\"] / ((df[\"height\"] / 100) ** 2)\n",
    "        \n",
    "        # Remove null tuples\n",
    "        records_with_nulls = df[\n",
    "            df[[\"weight\", \"height\", \"sex\", \"age\"]].isna().any(axis=1)\n",
    "        ]\n",
    "        df = df.dropna(subset=[\"height\", \"weight\", \"sex\"])\n",
    "        print(\n",
    "            f\"Number of records with NaN value in weight or height in {name}: {len(records_with_nulls)}\"\n",
    "        )\n",
    "\n",
    "        # Store the processed DataFrame in the new dictionary\n",
    "        processed_dfs[name] = df\n",
    "\n",
    "    return processed_dfs\n",
    "\n",
    "processed_dfs = preprocess(df_dict)\n",
    "# Define your mappings dictionary for each Caspian dataset\n",
    "mappings = {\n",
    "    'caspian_I_prev': {\n",
    "        \"Gorgan\": \"Golestan\",\n",
    "        \"Mashad\": \"Razavi Khorasan\",\n",
    "        \"ShahidBeheshti\": \"Tehran\",\n",
    "        \"Tabriz\": \"East Azerbaijan\",\n",
    "        \"Yazd\": \"Yazd\",\n",
    "        \"Rasht\": \"Gilan\"\n",
    "    },\n",
    "    'caspian_I_antro':{\n",
    "            \"Rasht\": \"Gilan\",\n",
    "            \"Arak\": \"Markazi\",\n",
    "            \"Ahvaz\": \"Khuzestan\",\n",
    "            \"Tabriz\": \"East Azerbaijan\",\n",
    "            \"Khoram_Abad\": \"Lorestan\",\n",
    "            \"Zahedan\": \"Sistan and Baluchestan\",\n",
    "            \"Zabol\": \"Sistan and Baluchestan\",\n",
    "            \"Semnan\": \"Semnan\",\n",
    "            \"Sanandaj\": \"Kurdistan\",\n",
    "            \"Gorgan\": \"Golestan\",\n",
    "            \"Mashad\": \"Razavi Khorasan\",\n",
    "            \"Yazd\": \"Yazd\",\n",
    "            \"Ealam\": \"Ilam\",\n",
    "            \"Qom\": \"Qom\",\n",
    "            \"Kermanshah\": \"Kermanshah\",\n",
    "            \"Kerman\": \"Kerman\",\n",
    "            \"Oroomieh\": \"West Azerbaijan\",\n",
    "            \"Ardebil\": \"Ardabil\",\n",
    "            \"Bandar_Abbas\": \"Hormozgan\",\n",
    "            \"Booshehr\": \"Bushehr\",\n",
    "            \"Zanjan\": \"Zanjan\",\n",
    "            \"ShahrKord\": \"Chaharmahal and Bakhtiari\",\n",
    "            \"ShahidBeheshti\": \"Tehran\"},\n",
    "        \n",
    "}\n",
    "university_to_province = mappings.get('caspian_I_prev')\n",
    "processed_dfs['caspian_I_prev']['university'] = processed_dfs['caspian_I_prev']['university'].replace(university_to_province)           \n",
    "# Ensure the 'university' column is of type string\n",
    "processed_dfs['caspian_I_prev']['university'] = processed_dfs['caspian_I_prev']['university'].astype(str)\n",
    "# Sort the DataFrame by the 'university' column\n",
    "processed_dfs['caspian_I_prev'] = processed_dfs['caspian_I_prev'].sort_values(by='university', ascending=True)\n",
    "\n",
    "university_to_province = mappings.get('caspian_I_antro')\n",
    "processed_dfs['caspian_I_antro']['university'] = processed_dfs['caspian_I_antro']['university'].replace(university_to_province)           \n",
    "# Ensure the 'university' column is of type string\n",
    "processed_dfs['caspian_I_antro']['university'] = processed_dfs['caspian_I_antro']['university'].astype(str)\n",
    "# Sort the DataFrame by the 'university' column\n",
    "processed_dfs['caspian_I_antro'] = processed_dfs['caspian_I_antro'].sort_values(by='university', ascending=True)\n",
    "\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 82 entries, 14543 to 7332\n",
      "Data columns (total 30 columns):\n",
      " #   Column      Non-Null Count  Dtype   \n",
      "---  ------      --------------  -----   \n",
      " 0   id          82 non-null     float64 \n",
      " 1   code        82 non-null     float64 \n",
      " 2   sex         82 non-null     category\n",
      " 3   university  82 non-null     object  \n",
      " 4   state       82 non-null     float64 \n",
      " 5   province    82 non-null     float64 \n",
      " 6   region      72 non-null     category\n",
      " 7   schoolType  82 non-null     category\n",
      " 8   grade       82 non-null     category\n",
      " 9   birthdat    82 non-null     float64 \n",
      " 10  age         82 non-null     float64 \n",
      " 11  fjob        81 non-null     category\n",
      " 12  feducati    82 non-null     category\n",
      " 13  mjob        82 non-null     category\n",
      " 14  meducati    81 non-null     category\n",
      " 15  househol    70 non-null     float64 \n",
      " 16  housing     82 non-null     category\n",
      " 17  weight      82 non-null     float64 \n",
      " 18  height      82 non-null     float64 \n",
      " 19  waist       82 non-null     float64 \n",
      " 20  hip         82 non-null     float64 \n",
      " 21  bmi         82 non-null     float64 \n",
      " 22  fbs_rec     0 non-null      category\n",
      " 23  tc_r        0 non-null      category\n",
      " 24  tg_rec      0 non-null      category\n",
      " 25  tc_rec      0 non-null      category\n",
      " 26  ldl_re      0 non-null      category\n",
      " 27  hdl_rec     0 non-null      category\n",
      " 28  bmi_rec     81 non-null     category\n",
      " 29  bmi1        82 non-null     float64 \n",
      "dtypes: category(16), float64(13), object(1)\n",
      "memory usage: 13.6+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "df1=processed_dfs['caspian_I_antro']\n",
    "df2=processed_dfs['caspian_I_next']\n",
    "\n",
    "column_to_exclude = \"id\"\n",
    "\n",
    "# Select all columns except the specified one\n",
    "matching_features = [col for col in df1.columns if col != column_to_exclude]\n",
    "\n",
    "# Merge the datasets on matching features\n",
    "merged_df = pd.merge(df1, df2, on=matching_features, suffixes=('_df1', '_df2'))\n",
    "\n",
    "# Group by 'id2' from df2 and collect 'id2' values from df1\n",
    "result = merged_df.groupby('id_df2')['id_df1'].apply(list).reset_index()\n",
    "\n",
    "# Rename columns for clarity\n",
    "result.columns = ['record_id_df2', 'records_id_df1']\n",
    "\n",
    "\n",
    "# Step 1: Flatten the list of `id2` values from `records_id2_df1`\n",
    "id2_to_remove = set([item for sublist in result['records_id_df1'] for item in sublist])\n",
    "\n",
    "# Step 2: Filter `caspian_IV` to exclude these `id2` values\n",
    "df_filtered = df1[~df1['id'].isin(id2_to_remove)]\n",
    "\n",
    "# Display the filtered DataFrame\n",
    "print(df_filtered.info())\n",
    "\n",
    "output_file = 'intersection_data_I-antro&I-data.xlsx'\n",
    "with pd.ExcelWriter(output_file) as writer:\n",
    "    result.to_excel(writer, index=False, sheet_name='ID2 Intersections')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4172    False\n",
       "4084    False\n",
       "4085    False\n",
       "4086    False\n",
       "4087    False\n",
       "        ...  \n",
       "9144    False\n",
       "9143    False\n",
       "9142    False\n",
       "9148    False\n",
       "8937    False\n",
       "Length: 20015, dtype: bool"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Returns a Series of True/False for each row, where True indicates a duplicate\n",
    "duplicated_rows = processed_dfs['caspian_I_antro'].duplicated()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20015, 30)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_dfs['caspian_I_antro'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20015, 30)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_dfs['caspian_I_next'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(82, 30)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_filtered.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CASPIAN_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
